{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c9509",
   "metadata": {},
   "source": [
    "# M3L2 Transformers Lab\n",
    "In this lab, we will practice how to download various models from the open source HuggingFace repository (https://huggingface.co/).  Please check out the website and click on the **Models** and **Datasets** tab to familiarize yourself with the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938062cf-31c6-456a-946f-8bf7070246a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e934c59-764b-42c8-8c8d-501700876d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy==1.11.4\n",
    "!pip install transformers\n",
    "!pip install typing_extensions==4.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64798fc",
   "metadata": {},
   "source": [
    "### Section 1.1 - Sentiment Analysis \n",
    "\n",
    "First, let's look at using a transformer for Sentiment Analysis.  This task will take in a sentence and classify it as positive or negative.  Some models will output other classes, such as \"neutral\" or other labels depending on how they were trained.  You can go to the huggingface website for each model and see what the expected output classes will be, along with tips on how to use these models.  \n",
    "\n",
    "The default classifier is \"distilbert-base-uncased-finetuned-sst-2-english\", which returns a 2 class output (positive or negative sentiment) of the sentence that you supply.  \n",
    "\n",
    "We will start with the simplest way to use a model, with a feature called a ***pipeline***.  These are pre-trained models, so there is no training necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(10)\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7116fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = classifier(\"I am mad.\")\n",
    "\n",
    "print(res) # tells you sentiment of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592d700",
   "metadata": {},
   "source": [
    "### Section 1.2 - Load different Sentiment Analysis Model\n",
    "We will see how to change the model.  This model was trained on financial data, and also on 3 classes - positive, negative and neutral.  These differences from the previous model will become apparent in the results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(10)\n",
    "classifier2 = pipeline(task='sentiment-analysis', model='ProsusAI/finbert') \n",
    "res = classifier2(\"I am mad.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c57ed",
   "metadata": {},
   "source": [
    "So the classifier doesn't get this right.  It thinks \"I am mad\" is a *positive* result.  There are 3 classes, so random guessing is 33%.  So here, it predicts positive by 37% or just better than random guessing.  \n",
    "\n",
    "However, if we were to use a prompt that is more financial, you might get better results: https://huggingface.co/ProsusAI/finbert?text=I+am+mad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7db43",
   "metadata": {},
   "source": [
    "### Section 2 - Text Generation\n",
    "In this section, let's explore how to use transformers for text generation, given a specific prompt.\n",
    "\n",
    "This is the default classifier for text generation, where you supply a seed and see what you get.  GPT2 is the default model that is loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4694c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(10)\n",
    "generator(\"Hello, I like data science because \", max_length=50, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24bbe5",
   "metadata": {},
   "source": [
    "Let's try another classifier.  ***Distilgpt2*** is a much smaller classifier.  Let's see how it does with the same prompt and seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(10)\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator(\"I love data science because,\", max_length=50, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae95af8",
   "metadata": {},
   "source": [
    "As you can see, the performance can be very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d020e5",
   "metadata": {},
   "source": [
    "### Section 3 - Fine tuning the model\n",
    "In this section, we will show how to fine tune a model to fit the data that is relevant to your application.\n",
    "\n",
    "We will be using a reduced BERT transformer called distilbert-base-uncased-finetuned-sst-2-english.  Documentation can be found here: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english.  This model was chosen becuase it is small in size while still being comparable in performance to the full BERT model.  The small size will make it easier to train on a single laptop in a reasonable amount of time.\n",
    "\n",
    "This model was trained on the *glue* and *sst2* datasets, which are made up of generalized language sentences and phrases.  \n",
    "\n",
    "Here are the steps we will be taking:\n",
    "- Load sentiment-analysis transformer and conduct baseline test\n",
    "- Train transformer on new dataset, IMDB, which is made up of movie reviews\n",
    "- Test transformer on same text as in baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and baseline performance\n",
    "set_seed(10)\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english' \n",
    "classifier = pipeline(\"text-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3481af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB database quote: \"I can't believe that those praising this movie herein aren't thinking of some other film.\"\n",
    "# This is reworded below so that we are not training and testing on the same words.  \n",
    "classifier(\"Your praise would be better for another film.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd79b4",
   "metadata": {},
   "source": [
    "Next let's retrain the classifier on the IMDB movie review dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bda924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a size from 0-25K.  Here, I'm choosing a small number for demonstration purposes\n",
    "test_size=50\n",
    "train_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd148f00-e147-49d4-abfa-815d96d2ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow==12.0.1 datasets==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b675329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset_train = dataset[\"train\"][0:train_size]  # Just take the training split for now\n",
    "print(dataset_train['text'][10])\n",
    "print(dataset_train['label'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a235ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Next we need to tokenize the new IMDB dataset in the format of the transformer\n",
    "'''\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using DistilBERT as it is 2.5x faster to train than the base BERT model.  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = tokenizer.batch_encode_plus(dataset_train[\"text\"], return_tensors=\"np\", \n",
    "                                             padding=True, max_length=512, truncation=True )\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels_train = np.array(dataset_train[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train the model with the new tokenized text'''\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5)) \n",
    "model.fit(tokenized_data, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa77a23",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b91fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"imdb\")\n",
    "dataset_test = dataset[\"test\"][0:test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokenize the test data'''\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using DistilBERT as it is 2.5x faster to train than the base BERT model.  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = tokenizer.batch_encode_plus(dataset_test[\"text\"], return_tensors=\"np\", \n",
    "                                             padding=True, max_length=512, truncation=True )\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_test_data = dict(tokenized_data)\n",
    "\n",
    "labels_test = np.array(dataset_test[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22071a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can do predictions like in Keras\n",
    "ypred = model.predict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs are in logits, so you need to use a softmax to get predictions\n",
    "import tensorflow as tf\n",
    "ypred_predictions = tf.nn.softmax(ypred.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aec8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use argmax to get the label depending on which class gets the maximum prediction\n",
    "y_test_pred_labels = np.argmax(ypred_predictions, axis=1)\n",
    "y_test_pred_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to the true data\n",
    "labels_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the overall accuracy\n",
    "model.evaluate(tokenized_test_data, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c4b0a",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "Using only 50 training and test observations, performance is low.  Also we only had 1 epoch.  If you have a GPU or a more powerful computing platform, you may want to use more observations and run multiple epochs to see if that improves performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41789be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokenize the test data'''\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using DistilBERT as it is 2.5x faster to train than the base BERT model.  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = tokenizer.encode_plus(\"Your praise would be better for another film.\", return_tensors=\"np\", \n",
    "                                             padding='max_length', max_length=512,truncation=True) #\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_test_data = dict(tokenized_data)\n",
    "\n",
    "labels_test = 0# np.array(dataset_test[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cae44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd595986",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(tokenized_test_data)\n",
    "ypred_predictions = tf.nn.softmax(ypred.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d11da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bf98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
